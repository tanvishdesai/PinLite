Paper ID,Section,Parameter,Value,,
M2TR (2022),I. Publication Details,Paper Title,M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection,,
M2TR (2022),I. Publication Details,Authors,"Junke Wang, Zuxuan Wu, Wenhao Ouyang, Xintong Han, Jingjing Chen, Ser-Nam Lim, Yu-Gang Jiang",,
M2TR (2022),I. Publication Details,Publication Year,2022,,
M2TR (2022),I. Publication Details,Publication Venue,ACM International Conference on Multimedia Retrieval (ICMR '22),,
M2TR (2022),I. Publication Details,Type of Paper,Conference Paper,,
M2TR (2022),II. Research Overview,Core Idea / Contribution,Proposes a Multi-modal Multi-scale Transformer that detects local inconsistencies at different scales and leverages frequency domain features to improve robustness against compression.,,
M2TR (2022),II. Research Overview,Problem Addressed,"Deepfakes have subtle artifacts at different scales (patches) and are often compressed, making RGB-only detection difficult.",,
M2TR (2022),II. Research Overview,Key Findings,Multi-scale patches capture local inconsistencies better; Frequency domain features are robust to JPEG compression; Proposed SR-DF dataset improves evaluation diversity.,,
M2TR (2022),III. Technical Deep Dive,Proposed Method Name,M2TR (Multi-modal Multi-scale TRansformer),,
M2TR (2022),III. Technical Deep Dive,Architecture,Two-stream architecture: RGB Stream (Multi-scale Transformer) and Frequency Stream (Learnable Frequency Filter).,,
M2TR (2022),III. Technical Deep Dive,Video Encoder,EfficientNet-b4 (Backbone) feeding into Transformer blocks.,,
M2TR (2022),III. Technical Deep Dive,Audio Encoder,"N/A (Uses Frequency Domain of images, not Audio).",,
M2TR (2022),III. Technical Deep Dive,Fusion Mechanism,Cross Modality Fusion (CMF) block using query-key-value mechanism to fuse RGB and Frequency features.,,
M2TR (2022),III. Technical Deep Dive,Loss Function(s),"Cross-entropy loss, Segmentation loss (auxiliary), Contrastive loss.",,
M2TR (2022),IV. Experimental Evaluation,Training Datasets,"FaceForensics++ (FF++), SR-DF (Proposed)",,
M2TR (2022),IV. Experimental Evaluation,Testing Datasets,"FF++, Celeb-DF, SR-DF, ForgeryNet",,
M2TR (2022),IV. Experimental Evaluation,Evaluation Metrics,"Accuracy (Acc), Area Under Curve (AUC), Mask-SSIM",,
M2TR (2022),IV. Experimental Evaluation,Accuracy,92.89% (FF++ LQ),97.93% (FF++ HQ),
M2TR (2022),IV. Experimental Evaluation,AUC,99.92% (FF++ Raw),99.8% (Celeb-DF),
M2TR (2022),IV. Experimental Evaluation,F1-Score,Not explicitly reported in summary tables (focus on AUC/Acc),,
M2TR (2022),IV. Experimental Evaluation,Other Results,Outperforms state-of-the-art (F3-Net,MaDD) on Celeb-DF and SR-DF.,
M2TR (2022),V. Efficiency Metrics (PIN-Lite Specific),Model Size (MB),Not explicitly stated (Standard EfficientNet-b4 is ~70MB+ plus Transformers).,,
M2TR (2022),V. Efficiency Metrics (PIN-Lite Specific),Parameters (M),Not explicitly stated (EfficientNet-b4 is ~19M params; Transformer adds overhead).,,
M2TR (2022),V. Efficiency Metrics (PIN-Lite Specific),FLOPs (G),Not reported.,,
M2TR (2022),V. Efficiency Metrics (PIN-Lite Specific),Inference Time (ms),Not reported.,,
M2TR (2022),V. Efficiency Metrics (PIN-Lite Specific),Compression Techniques Used,None mentioned (Focus is on robustness to compressed *inputs*,not model compression).,
M2TR (2022),V. Efficiency Metrics (PIN-Lite Specific),Edge Deployment Discussed,No.,,
M2TR (2022),VI. Critical Analysis,Strengths,Robust to compression due to frequency stream; Multi-scale approach captures subtle artifacts; New high-quality dataset (SR-DF).,,
M2TR (2022),VI. Critical Analysis,Drawbacks & Limitations,Complex architecture (Transformer + CNN); potentially high computational cost compared to pure CNNs.,,
M2TR (2022),VI. Critical Analysis,Future Work,Not explicitly detailed in snippets (Generalization is a focus).,,
M2TR (2022),VII. Relevance to PIN-Lite,Comparison Points,High accuracy benchmark; use of frequency domain could be adapted for lightweight models.,,
M2TR (2022),VII. Relevance to PIN-Lite,Key Differences from Our Approach,M2TR uses heavy Transformer blocks; PIN-Lite likely targets lightweight/edge efficiency.,,
M2TR (2022),VII. Relevance to PIN-Lite,Citation Context,Standard benchmark for detecting compression-resistant deepfakes.,,
DF-P2E (2025),I. Publication Details,Paper Title,"From Prediction to Explanation: Multimodal, Explainable, and Interactive Deepfake Detection Framework for Non-Expert Users",,
DF-P2E (2025),I. Publication Details,Authors,"Shahroz Tariq, Simon S. Woo, Priyanka Singh, Irena Irmalasari, Saakshi Gupta, Dev Gupta",,
DF-P2E (2025),I. Publication Details,Publication Year,2025,,
DF-P2E (2025),I. Publication Details,Publication Venue,ACM International Conference on Multimedia (MM '25),,
DF-P2E (2025),I. Publication Details,Type of Paper,Conference Paper,,
DF-P2E (2025),II. Research Overview,Core Idea / Contribution,A framework providing natural language explanations for deepfake detection using VLM/LLMs,bridging the gap for non-expert users.,
DF-P2E (2025),II. Research Overview,Problem Addressed,Existing detectors are black-boxes; heatmaps (Grad-CAM) are insufficient for non-experts to understand *why* an image is fake.,,
DF-P2E (2025),II. Research Overview,Key Findings,Multimodal explanations (Visual + Text) increase user trust; CLIP-large backbone outperforms XceptionNet; LLM refinement creates human-readable forensic reports.,,
DF-P2E (2025),III. Technical Deep Dive,Proposed Method Name,DF-P2E (Deepfake: Prediction to Explanation),,
DF-P2E (2025),III. Technical Deep Dive,Architecture,"Modular: 1. Detector (CLIP-large), 2. Captioning (BLIP-large), 3. Narrative Refinement (LLaMA-3.2-11B).",,
DF-P2E (2025),III. Technical Deep Dive,Video Encoder,CLIP-large (Vision Transformer based).,,
DF-P2E (2025),III. Technical Deep Dive,Audio Encoder,N/A (Focus on Visual Explanations).,,
DF-P2E (2025),III. Technical Deep Dive,Fusion Mechanism,Sequential pipeline: Image -> Heatmap -> Caption -> LLM Narrative.,,
DF-P2E (2025),III. Technical Deep Dive,Loss Function(s),Standard Classification losses for detector; Causal Language Modeling losses for Captioning/LLM.,,
DF-P2E (2025),IV. Experimental Evaluation,Training Datasets,"Fine-tuned on DF40 (FaceForensics++, Celeb-DF, UADFV, etc. combined).",,
DF-P2E (2025),IV. Experimental Evaluation,Testing Datasets,DF40 Benchmark (subsets: Face Swap,Reenactment,Synthesis).
DF-P2E (2025),IV. Experimental Evaluation,Evaluation Metrics,"AUC (Detection); BLEU, METEOR, CIDEr (Captioning); Human Evaluation (Likert scale).",,
DF-P2E (2025),IV. Experimental Evaluation,Accuracy,Not primarily reported (AUC focused).,,
DF-P2E (2025),IV. Experimental Evaluation,AUC,0.913 (Average on DF40 with CLIP-large).,,
DF-P2E (2025),IV. Experimental Evaluation,F1-Score,Not reported.,,
DF-P2E (2025),IV. Experimental Evaluation,Other Results,Captioning Latency: ~1845s for 100 images (BLIP-large). Human Eval: 4.5/5 for usefulness.,,
DF-P2E (2025),V. Efficiency Metrics (PIN-Lite Specific),Model Size (MB),Large. CLIP-large + BLIP-large + LLaMA-3.2 (11B parameters). ~11GB VRAM for LLM alone.,,
DF-P2E (2025),V. Efficiency Metrics (PIN-Lite Specific),Parameters (M),">11,000M (LLaMA-3.2 is 11B).",,
DF-P2E (2025),V. Efficiency Metrics (PIN-Lite Specific),FLOPs (G),Very High.,,
DF-P2E (2025),V. Efficiency Metrics (PIN-Lite Specific),Inference Time (ms),High latency. Total time for 100 images ~2800s (approx 28s per image) with heavy models.,,
DF-P2E (2025),V. Efficiency Metrics (PIN-Lite Specific),Compression Techniques Used,Quantization (4-bit) used for LLaMA-3.2 to fit memory.,,
DF-P2E (2025),V. Efficiency Metrics (PIN-Lite Specific),Edge Deployment Discussed,Implied limitation (Latency/Size discussed).,,
DF-P2E (2025),VI. Critical Analysis,Strengths,Excellent interpretability; User-centric; High generalization (CLIP backbone).,,
DF-P2E (2025),VI. Critical Analysis,Drawbacks & Limitations,High inference latency; computational expense; risk of LLM hallucinations.,,
DF-P2E (2025),VI. Critical Analysis,Future Work,Optimizing computational efficiency; expanding to audio-visual generation.,,
DF-P2E (2025),VII. Relevance to PIN-Lite,Comparison Points,"Example of ""heavy"" interpretable model vs PIN-Lite's presumed efficiency.",,
DF-P2E (2025),VII. Relevance to PIN-Lite,Key Differences from Our Approach,DF-P2E prioritizes explanation over speed/size; PIN-Lite likely prioritizes efficiency.,,
DF-P2E (2025),VII. Relevance to PIN-Lite,Citation Context,State-of-the-art for XAI in deepfake detection.,,
CAD (2025),I. Publication Details,Paper Title,CAD: A General Multimodal Framework for Video Deepfake Detection via Cross-Modal Alignment and Distillation,,
CAD (2025),I. Publication Details,Authors,"Yuxuan Du, Zhendong Wang, Yuhao Luo, Caiyong Piao, Zhiyuan Yan, Hao Li, Li Yuan",,
CAD (2025),I. Publication Details,Publication Year,2025,,
CAD (2025),I. Publication Details,Publication Venue,arXiv / Under Review,,
CAD (2025),I. Publication Details,Type of Paper,Preprint,,
CAD (2025),II. Research Overview,Core Idea / Contribution,Unifies detection of modality-specific artifacts and semantic misalignments (e.g.,lip-sync) using cross-modal alignment and distillation.,
CAD (2025),II. Research Overview,Problem Addressed,Existing methods focus either on visual artifacts OR semantic (lip-speech) mismatch; neglecting either limits performance on multimodal deepfakes.,,
CAD (2025),II. Research Overview,Key Findings,Modality-specific and shared cues are complementary; Cross-modal distillation improves single-modality performance; Achieves SOTA on IDForge.,,
CAD (2025),III. Technical Deep Dive,Proposed Method Name,CAD (Cross-Modal Alignment and Distillation),,
CAD (2025),III. Technical Deep Dive,Architecture,Dual-path: Visual Encoder (CLIP ViT) + Audio Encoder (Whisper). Features aligned via Cross-Attention.,,
CAD (2025),III. Technical Deep Dive,Video Encoder,CLIP ViT-Base-16 (Frozen + Adapters).,,
CAD (2025),III. Technical Deep Dive,Audio Encoder,Whisper-Small (Frozen + LoRA).,,
CAD (2025),III. Technical Deep Dive,Fusion Mechanism,Cross-Modal Alignment (Cross-Attention) and Cross-Modal Distillation (SimSiam).,,
CAD (2025),III. Technical Deep Dive,Loss Function(s),"KL Divergence (Alignment), SimSiam Loss (Distillation), Cross-Entropy (Detection).",,
CAD (2025),IV. Experimental Evaluation,Training Datasets,"FakeAVCeleb, IDForge-v2",,
CAD (2025),IV. Experimental Evaluation,Testing Datasets,"FakeAVCeleb, IDForge-v2, FaceShifter, Celeb-DF",,
CAD (2025),IV. Experimental Evaluation,Evaluation Metrics,"Acc, AUC, Average Precision (AP)",,
CAD (2025),IV. Experimental Evaluation,Accuracy,99.0% on FakeAVCeleb.,,
CAD (2025),IV. Experimental Evaluation,AUC,"99.96% on IDForge, 99.6% on FakeAVCeleb.",,
CAD (2025),IV. Experimental Evaluation,F1-Score,Not reported.,,
CAD (2025),IV. Experimental Evaluation,Other Results,Strong cross-manipulation generalization (AUC 96.9% on avg).,,
CAD (2025),V. Efficiency Metrics (PIN-Lite Specific),Model Size (MB),Moderate. Uses Base/Small transformers.,,
CAD (2025),V. Efficiency Metrics (PIN-Lite Specific),Parameters (M),CLIP-ViT-B/16 (~86M) + Whisper-Small (~244M).,,
CAD (2025),V. Efficiency Metrics (PIN-Lite Specific),FLOPs (G),Not reported.,,
CAD (2025),V. Efficiency Metrics (PIN-Lite Specific),Inference Time (ms),Not reported.,,
CAD (2025),V. Efficiency Metrics (PIN-Lite Specific),Compression Techniques Used,Uses Frozen backbones with LoRA/Adapters (Parameter Efficient Fine-Tuning).,,
CAD (2025),V. Efficiency Metrics (PIN-Lite Specific),Edge Deployment Discussed,Not explicitly.,,
CAD (2025),VI. Critical Analysis,Strengths,High accuracy; handles both visual and audio-visual fakes; efficient tuning (LoRA).,,
CAD (2025),VI. Critical Analysis,Drawbacks & Limitations,Reliance on large pre-trained models (CLIP/Whisper) may limit speed.,,
CAD (2025),VI. Critical Analysis,Future Work,Explore larger models; integrate autoregressive LLMs.,,
CAD (2025),VII. Relevance to PIN-Lite,Comparison Points,Demonstrates value of multimodal features; distillation part is relevant for compression.,,
CAD (2025),VII. Relevance to PIN-Lite,Key Differences from Our Approach,CAD relies on heavy audio-visual pre-trained encoders.,,
CAD (2025),VII. Relevance to PIN-Lite,Citation Context,SOTA for Multimodal (Audio-Visual) Deepfake Detection.,,
XDistillation (2021),I. Publication Details,Paper Title,Learning Interpretation with Explainable Knowledge Distillation,,
XDistillation (2021),I. Publication Details,Authors,"Raed Alharbi, Minh N. Vu, My T. Thai",,
XDistillation (2021),I. Publication Details,Publication Year,2021,,
XDistillation (2021),I. Publication Details,Publication Venue,IEEE International Conference on Big Data,,
XDistillation (2021),I. Publication Details,Type of Paper,Conference Paper,,
XDistillation (2021),II. Research Overview,Core Idea / Contribution,"Distilling not just logits but also ""Explanations"" (interpretability) from teacher to student using a Convolutional Autoencoder (CAE).",,
XDistillation (2021),II. Research Overview,Problem Addressed,Standard KD transfers performance but not interpretability; Student explanations often diverge from Teacher's.,,
XDistillation (2021),II. Research Overview,Key Findings,XDistillation produces students that match teacher accuracy AND explanation consistency better than standard KD.,,
XDistillation (2021),III. Technical Deep Dive,Proposed Method Name,XDistillation,,
XDistillation (2021),III. Technical Deep Dive,Architecture,Teacher (VGG16) -> Student (VGG7). Explainable Feature Fusion via CAE.,,
XDistillation (2021),III. Technical Deep Dive,Video Encoder,CNN (VGG variants used).,,
XDistillation (2021),III. Technical Deep Dive,Audio Encoder,N/A,,
XDistillation (2021),III. Technical Deep Dive,Fusion Mechanism,Fusion of Student features with approximated Teacher explanations.,,
XDistillation (2021),III. Technical Deep Dive,Loss Function(s),"Knowledge Distillation Loss (KL), Classification Loss, Feature Reduction Penalty.",,
XDistillation (2021),IV. Experimental Evaluation,Training Datasets,CIFAR-10,MNIST,
XDistillation (2021),IV. Experimental Evaluation,Testing Datasets,CIFAR-10,MNIST,
XDistillation (2021),IV. Experimental Evaluation,Evaluation Metrics,Accuracy,Explanation MSE,Overlap Score.
XDistillation (2021),IV. Experimental Evaluation,Accuracy,90.9% (Student) vs 90.2% (Standard KD) on CIFAR-10.,,
XDistillation (2021),IV. Experimental Evaluation,AUC,N/A,,
XDistillation (2021),IV. Experimental Evaluation,F1-Score,N/A,,
XDistillation (2021),IV. Experimental Evaluation,Other Results,Explanation consistency (GradCAM overlap) improved by ~6%.,,
XDistillation (2021),V. Efficiency Metrics (PIN-Lite Specific),Model Size (MB),Reduced (Teacher VGG16 to Student VGG7).,,
XDistillation (2021),V. Efficiency Metrics (PIN-Lite Specific),Parameters (M),Teacher: 14.7M -> Student: 3.5M (Significant compression).,,
XDistillation (2021),V. Efficiency Metrics (PIN-Lite Specific),FLOPs (G),Not reported.,,
XDistillation (2021),V. Efficiency Metrics (PIN-Lite Specific),Inference Time (ms),Not reported.,,
XDistillation (2021),V. Efficiency Metrics (PIN-Lite Specific),Compression Techniques Used,Knowledge Distillation (KD).,,
XDistillation (2021),V. Efficiency Metrics (PIN-Lite Specific),Edge Deployment Discussed,Implied (focus on lightweight/low computing resources).,,
XDistillation (2021),VI. Critical Analysis,Strengths,"Transfers ""reasoning"" not just probability; Improves student accuracy over standard KD.",,
XDistillation (2021),VI. Critical Analysis,Drawbacks & Limitations,Requires training auxiliary CAE; Evaluated on simple datasets (CIFAR/MNIST) not Deepfakes.,,
XDistillation (2021),VI. Critical Analysis,Future Work,N/A,,
XDistillation (2021),VII. Relevance to PIN-Lite,Comparison Points,Directly relevant for Model Compression (KD).,,
XDistillation (2021),VII. Relevance to PIN-Lite,Key Differences from Our Approach,Uses generative approach (CAE) to approximate explanations.,,
XDistillation (2021),VII. Relevance to PIN-Lite,Citation Context,Methodology for compressing models while retaining interpretability.,,
e2KD (2024),I. Publication Details,Paper Title,Good Teachers Explain: Explanation-Enhanced Knowledge Distillation,,
e2KD (2024),I. Publication Details,Authors,"Amin Parchami-Araghi, Moritz Böhle, Sukrut Rao, Bernt Schiele",,
e2KD (2024),I. Publication Details,Publication Year,"2024 (arXiv), 2022 (CVPR)",,
e2KD (2024),I. Publication Details,Publication Venue,CVPR (Conference on Computer Vision and Pattern Recognition),,
e2KD (2024),I. Publication Details,Type of Paper,Conference Paper,,
e2KD (2024),II. Research Overview,Core Idea / Contribution,e²KD: Forces student explanations (GradCAM/B-cos) to match Teacher's using a cosine similarity loss.,,
e2KD (2024),II. Research Overview,Problem Addressed,Students in KD often learn different input features than teachers (e.g.,background vs object),leading to poor OOD generalization.
e2KD (2024),II. Research Overview,Key Findings,"e²KD improves accuracy, student-teacher agreement, and robustness to limited data/distribution shifts.",,
e2KD (2024),III. Technical Deep Dive,Proposed Method Name,e²KD (Explanation-Enhanced Knowledge Distillation),,
e2KD (2024),III. Technical Deep Dive,Architecture,Teacher (ResNet34/50/DenseNet) -> Student (ResNet18/ViT).,,
e2KD (2024),III. Technical Deep Dive,Video Encoder,ResNet / ViT / DenseNet (Image models).,,
e2KD (2024),III. Technical Deep Dive,Audio Encoder,N/A,,
e2KD (2024),III. Technical Deep Dive,Fusion Mechanism,Loss-based fusion (Logit Loss + Explanation Similarity Loss).,,
e2KD (2024),III. Technical Deep Dive,Loss Function(s),KL Divergence (KD) + Cosine Similarity (Explanation Loss).,,
e2KD (2024),IV. Experimental Evaluation,Training Datasets,ImageNet,Waterbirds,Pascal VOC.
e2KD (2024),IV. Experimental Evaluation,Testing Datasets,ImageNet,Waterbirds (OOD).,
e2KD (2024),IV. Experimental Evaluation,Evaluation Metrics,Top-1 Accuracy,Teacher-Student Agreement (%).,
e2KD (2024),IV. Experimental Evaluation,Accuracy,71.8% (ImageNet ResNet18 Student) vs 69.8% (Baseline).,,
e2KD (2024),IV. Experimental Evaluation,AUC,N/A,,
e2KD (2024),IV. Experimental Evaluation,F1-Score,N/A,,
e2KD (2024),IV. Experimental Evaluation,Other Results,High robustness on limited data (50 shots: +5.1% acc gain over vanilla KD).,,
e2KD (2024),V. Efficiency Metrics (PIN-Lite Specific),Model Size (MB),Compressed (Teacher -> Student).,,
e2KD (2024),V. Efficiency Metrics (PIN-Lite Specific),Parameters (M),ResNet34 (~21M) -> ResNet18 (~11M).,,
e2KD (2024),V. Efficiency Metrics (PIN-Lite Specific),FLOPs (G),Reduced (Student is lighter).,,
e2KD (2024),V. Efficiency Metrics (PIN-Lite Specific),Inference Time (ms),Not reported.,,
e2KD (2024),V. Efficiency Metrics (PIN-Lite Specific),Compression Techniques Used,Knowledge Distillation (KD).,,
e2KD (2024),V. Efficiency Metrics (PIN-Lite Specific),Edge Deployment Discussed,Implied (Model Compression focus).,,
e2KD (2024),VI. Critical Analysis,Strengths,"Model-agnostic; Parameter-free addition; Improves ""Right for Right Reasons"".",,
e2KD (2024),VI. Critical Analysis,Drawbacks & Limitations,"Computational cost of generating explanations during training (mitigated by """"frozen"""" explanations).",,
e2KD (2024),VI. Critical Analysis,Future Work,N/A,,
e2KD (2024),VII. Relevance to PIN-Lite,Comparison Points,Excellent technique for ensuring a lightweight PIN-Lite model retains the robustness of a heavy teacher.,,
e2KD (2024),VII. Relevance to PIN-Lite,Key Differences from Our Approach,Focuses on explanation consistency rather than just accuracy.,,
e2KD (2024),VII. Relevance to PIN-Lite,Citation Context,Key reference for robust model compression/distillation.,,
,,,,,
,,,,,
,,,,,
Section,Parameter,Value,,,
I. Publication Details,Paper Title,AV-Lip-Sync+: Leveraging AV-HuBERT to Exploit Multimodal Inconsistency for Deepfake Detection of Frontal Face Videos,,,
I. Publication Details,Authors,"Sahibzada Adil Shahzad, Ammarah Hashmi, Yan-Tsung Peng, Yu Tsao, Hsin-Min Wang",,,
I. Publication Details,Publication Year,2025,,,
I. Publication Details,Publication Venue,IEEE Transactions on Human-Machine Systems (THMS),,,
I. Publication Details,Type of Paper,Journal Article,,,
II. Research Overview,Core Idea / Contribution,"Proposes a multimodal deepfake detection method using SSL pre-trained AV-HuBERT to exploit audio-visual inconsistencies, enhanced by a Video Vision Transformer (ViViT) for facial artifacts.",,,
II. Research Overview,Problem Addressed,"Detection of high-quality multimodal deepfakes where unimodal detectors fail, specifically focusing on synchronization between lip movements and speech.",,,
II. Research Overview,Key Findings,The model achieves state-of-the-art performance on FakeAVCeleb (99.29%) and DeepfakeTIMIT. Adding the face encoder (ViViT) significantly improves detection of visual-only manipulations like Faceswap.,,,
III. Technical Deep Dive,Proposed Method Name,AV-Lip-Sync+ (and AV-Lip-Sync+ with FE),,,
III. Technical Deep Dive,Architecture,AV-HuBERT (Visual & Acoustic feature extraction) + ResNet-18 (Lip features) + Transformer Encoder + MS-TCN + ViViT (Face Encoder),,,
III. Technical Deep Dive,Video Encoder,"ResNet-18 (Lip), ViViT (Full Face)",,,
III. Technical Deep Dive,Audio Encoder,"AV-HuBERT (Shared SSL model), FFN for acoustic features",,,
III. Technical Deep Dive,Fusion Mechanism,Concatenation of AV-HuBERT embeddings and Sync-Check module outputs; Late fusion with ViViT features.,,,
III. Technical Deep Dive,Loss Function(s),Cross-Entropy Loss,,,
IV. Experimental Evaluation,Training Datasets,FakeAVCeleb (balanced with VoxCeleb1 real videos),,,
IV. Experimental Evaluation,Testing Datasets,"FakeAVCeleb (various subsets), DeepfakeTIMIT",,,
IV. Experimental Evaluation,Evaluation Metrics,"Precision, Recall, F1-score, Accuracy, AUC",,,
IV. Experimental Evaluation,Accuracy,99.29% (FakeAVCeleb Test-set-2),,,
IV. Experimental Evaluation,AUC,99.98% (DeepfakeTIMIT HQ),,,
IV. Experimental Evaluation,F1-Score,99.28% (FakeAVCeleb),,,
IV. Experimental Evaluation,Other Results,Robust against partial occlusion and cross-dataset generalization.,,,
V. Efficiency Metrics (PIN-Lite Specific),Model Size (MB),Not Reported,,,
V. Efficiency Metrics (PIN-Lite Specific),Parameters (M),"132.85M (Base), 249.37M (with Face Encoder)",,,
V. Efficiency Metrics (PIN-Lite Specific),FLOPs (G),Not Reported,,,
V. Efficiency Metrics (PIN-Lite Specific),Inference Time (ms),"70ms (Base), 140ms (with Face Encoder) per video sample on RTX 2080 Ti",,,
V. Efficiency Metrics (PIN-Lite Specific),Compression Techniques Used,None (Uses large pre-trained SSL models),,,
V. Efficiency Metrics (PIN-Lite Specific),Edge Deployment Discussed,No (Implies high computational cost),,,
VI. Critical Analysis,Strengths,"High accuracy, effectively handles multimodal inconsistency, robust to specific manipulation types (Faceswap) due to added face encoder.",,,
VI. Critical Analysis,Drawbacks & Limitations,"High parameter count (approx 250M), relies on computationally expensive Transformers (ViViT, AV-HuBERT). Requires frontal faces.",,,
VI. Critical Analysis,Future Work,Improving generalizability in multimodal settings.,,,
VII. Relevance to PIN-Lite,Comparison Points,High-performance baseline that likely requires compression (pruning/quantization) for PIN-Lite objectives.,,,
VII. Relevance to PIN-Lite,Key Differences from Our Approach,Focuses on maximizing accuracy via large SSL models rather than efficiency/compression.,,,
VII. Relevance to PIN-Lite,Citation Context,Example of SOTA multimodal detection requiring heavy compute.,,,
,,,,,
I. Publication Details,Paper Title,"Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability",,,
I. Publication Details,Authors,"Daniel Hendriks, Philipp Spitzer, Niklas Kühl, Gerhard Satzger",,,
I. Publication Details,Publication Year,2025,,,
I. Publication Details,Publication Venue,IEEE Journal (Journal of LaTeX Class Files),,,
I. Publication Details,Type of Paper,Journal Article,,,
II. Research Overview,Core Idea / Contribution,"Systematic comparison of Knowledge Distillation (KD) methods for LLMs, introducing 'critique-revision prompting' for data generation and combining multitask with counterfactual training.",,,
II. Research Overview,Problem Addressed,Deploying reasoning capabilities of LLMs in resource-constrained environments while maintaining performance and explainability.,,,
II. Research Overview,Key Findings,Multitask training yields best performance. Critique-revision prompting improves explainability (quality of explanations) but not necessarily accuracy. Small models (T5) can rival larger teachers (PaLM) if trained well.,,,
III. Technical Deep Dive,Proposed Method Name,Critique-Revision Prompting + Multitask/Counterfactual Training,,,
III. Technical Deep Dive,Architecture,"Teacher: LLaMA-2-13B. Student: T5-base, T5-large",,,
III. Technical Deep Dive,Video Encoder,N/A (NLP Paper),,,
III. Technical Deep Dive,Audio Encoder,N/A (NLP Paper),,,
III. Technical Deep Dive,Fusion Mechanism,N/A,,,
III. Technical Deep Dive,Loss Function(s),"Combined loss: L_answer + L_explanation (Multitask), plus Counterfactual loss components.",,,
IV. Experimental Evaluation,Training Datasets,Commonsense Question Answering (CQA),,,
IV. Experimental Evaluation,Testing Datasets,Commonsense Question Answering (CQA),,,
IV. Experimental Evaluation,Evaluation Metrics,"Accuracy, Human-grounded explainability (Plausibility, Understandability, Completeness, etc.)",,,
IV. Experimental Evaluation,Accuracy,~60-75% range (Visual estimation from plots),,,
IV. Experimental Evaluation,AUC,N/A,,,
IV. Experimental Evaluation,F1-Score,N/A,,,
IV. Experimental Evaluation,Other Results,Critique-revision improved 'Completeness' and 'Contrastiveness' of explanations in human study.,,,
V. Efficiency Metrics (PIN-Lite Specific),Model Size (MB),Not Reported,,,
V. Efficiency Metrics (PIN-Lite Specific),Parameters (M),"220M (T5-base), 770M (T5-large)",,,
V. Efficiency Metrics (PIN-Lite Specific),FLOPs (G),Not Reported,,,
V. Efficiency Metrics (PIN-Lite Specific),Inference Time (ms),Not Reported (Focus on training time/convergence),,,
V. Efficiency Metrics (PIN-Lite Specific),Compression Techniques Used,"Knowledge Distillation (Teacher-Student), Small Architectures (T5)",,,
V. Efficiency Metrics (PIN-Lite Specific),Edge Deployment Discussed,"Yes, explicitly motivated by edge devices/mobile phones.",,,
VI. Critical Analysis,Strengths,Human-grounded evaluation of explainability. Novel data generation prompting strategy. Detailed statistical analysis of training methods.,,,
VI. Critical Analysis,Drawbacks & Limitations,Evaluation limited to one dataset (CQA). Critique-revision increased sequence length without boosting accuracy.,,,
VI. Critical Analysis,Future Work,"Apply to other tasks/datasets, combine with other compression methods.",,,
VII. Relevance to PIN-Lite,Comparison Points,Directly addresses 'Shrinking' models (KD) while maintaining capability (PIN-Lite core goal).,,,
VII. Relevance to PIN-Lite,Key Differences from Our Approach,"Focuses on NLP/Reasoning rather than Vision/Deepfake, but KD principles transfer.",,,
VII. Relevance to PIN-Lite,Citation Context,"Reference for KD methodologies and trade-offs between size, performance, and explainability.",,,
,,,,,
I. Publication Details,Paper Title,A Brief Review for Compression and Transfer Learning Techniques in DeepFake Detection,,,
I. Publication Details,Authors,"Andreas Karathanasis, John Violos, Ioannis Kompatsiaris, Symeon Papadopoulos",,,
I. Publication Details,Publication Year,2025,,,
I. Publication Details,Publication Venue,arXiv / Conference (implied),,,
I. Publication Details,Type of Paper,Review / Experimental Paper,,,
II. Research Overview,Core Idea / Contribution,"Investigates combining compression (Pruning, KD, Quantization) with Transfer Learning for Deepfake detection on edge devices.",,,
II. Research Overview,Problem Addressed,Deepfake detectors are too heavy for edge computing. Need to reduce size/latency without losing accuracy or generalization.,,,
II. Research Overview,Key Findings,90% compression is possible with preserved accuracy if training/testing data are from the same generator. Cross-generator performance drops significantly (domain generalization issue). KD often outperforms pruning.,,,
III. Technical Deep Dive,Proposed Method Name,"Comparative analysis of CPF, CKD, CQ, TL+FT, TL+KD, TL+Adapters, etc.",,,
III. Technical Deep Dive,Architecture,Base: VGG-like (4.5M params). Student models for KD.,,,
III. Technical Deep Dive,Video Encoder,VGG-based CNN,,,
III. Technical Deep Dive,Audio Encoder,N/A (Image only),,,
III. Technical Deep Dive,Fusion Mechanism,N/A,,,
III. Technical Deep Dive,Loss Function(s),Standard Cross-Entropy (implied for classification),,,
IV. Experimental Evaluation,Training Datasets,"Synthbuster, RAISE, ForenSynths (ProGAN)",,,
IV. Experimental Evaluation,Testing Datasets,"Synthbuster, ForenSynths (StarGAN, WhichFaceIsReal, DeepFake)",,,
IV. Experimental Evaluation,Evaluation Metrics,"Accuracy, F1-Score, Compression Time, Transfer Time",,,
IV. Experimental Evaluation,Accuracy,High (>90%) on same-domain; drops significantly on cross-domain.,,,
IV. Experimental Evaluation,AUC,Not Reported,,,
IV. Experimental Evaluation,F1-Score,Reported in plots (varies by compression rate),,,
IV. Experimental Evaluation,Other Results,Quantization limits GPU acceleration benefits. Adapters work best at end of conv layers for small models.,,,
V. Efficiency Metrics (PIN-Lite Specific),Model Size (MB),Base ~4.5M params. Compressed 60-90%.,,,
V. Efficiency Metrics (PIN-Lite Specific),Parameters (M),"~4.5M (Base), <0.5M (Compressed)",,,
V. Efficiency Metrics (PIN-Lite Specific),FLOPs (G),Not Reported,,,
V. Efficiency Metrics (PIN-Lite Specific),Inference Time (ms),Not Reported,,,
V. Efficiency Metrics (PIN-Lite Specific),Compression Techniques Used,"Pruning, Knowledge Distillation, Quantization (int8), Adapters.",,,
V. Efficiency Metrics (PIN-Lite Specific),Edge Deployment Discussed,"Yes, primary motivation.",,,
VI. Critical Analysis,Strengths,Comprehensive combination of compression + transfer learning. Highlighting the domain generalization gap in compressed models.,,,
VI. Critical Analysis,Drawbacks & Limitations,Used a relatively simple VGG base model rather than SOTA Transformers. Quantization results were mixed.,,,
VI. Critical Analysis,Future Work,Address domain generalization in compressed models.,,,
VII. Relevance to PIN-Lite,Comparison Points,Directly aligns with PIN-Lite's mission of efficient Deepfake detection.,,,
VII. Relevance to PIN-Lite,Key Differences from Our Approach,"Focuses on image-only Deepfakes (CNNs), whereas PIN-Lite may target video/multimodal.",,,
VII. Relevance to PIN-Lite,Citation Context, foundational reference for compression benchmarks (Pruning vs KD) in Deepfake domain.,,,
,,,,,
I. Publication Details,Paper Title,Interpretability-Aware Pruning for Efficient Medical Image Analysis,,,
I. Publication Details,Authors,"Nikita Malik, Pratinav Seth, Neeraj K. Singh, Chintan Chitroda, Vinay K. Sankarapu",,,
I. Publication Details,Publication Year,2025,,,
I. Publication Details,Publication Venue,arXiv,,,
I. Publication Details,Type of Paper,Conference/Preprint,,,
II. Research Overview,Core Idea / Contribution,"Uses XAI attribution methods (LRP, DL-Backtrace, IG) to determine neuron importance for unstructured pruning.",,,
II. Research Overview,Problem Addressed,Reducing computational complexity of medical imaging models while maintaining accuracy and increasing transparency/trust.,,,
II. Research Overview,Key Findings,LRP is the best attribution method for pruning. ViT models are highly robust (up to 85% pruning). Clustering-based sampling improves relevance score computation.,,,
III. Technical Deep Dive,Proposed Method Name,Interpretability-Aware Pruning,,,
III. Technical Deep Dive,Architecture,"VGG19, ResNet50, Vision Transformer (ViT-B/16)",,,
III. Technical Deep Dive,Video Encoder,N/A (Medical Imaging),,,
III. Technical Deep Dive,Audio Encoder,N/A,,,
III. Technical Deep Dive,Fusion Mechanism,N/A,,,
III. Technical Deep Dive,Loss Function(s),Cross-Entropy (for baseline training),,,
IV. Experimental Evaluation,Training Datasets,"MURA, KVASIR, CPN, FETAL Planes",,,
IV. Experimental Evaluation,Testing Datasets,"MURA, KVASIR, CPN, FETAL Planes",,,
IV. Experimental Evaluation,Evaluation Metrics,Accuracy vs Pruning Rate,,,
IV. Experimental Evaluation,Accuracy,Maintains baseline accuracy up to ~40-50% pruning (CNNs) and ~80% (ViTs).,,,
IV. Experimental Evaluation,AUC,Not Reported,,,
IV. Experimental Evaluation,F1-Score,Not Reported,,,
IV. Experimental Evaluation,Other Results,Clustering sampling outperforms random/confidence sampling for calculating neuron importance.,,,
V. Efficiency Metrics (PIN-Lite Specific),Model Size (MB),Not Reported,,,
V. Efficiency Metrics (PIN-Lite Specific),Parameters (M),Varies (ViT-B/16 is ~86M). Sparsity up to 85%.,,,
V. Efficiency Metrics (PIN-Lite Specific),FLOPs (G),Not Reported,,,
V. Efficiency Metrics (PIN-Lite Specific),Inference Time (ms),Not Reported,,,
V. Efficiency Metrics (PIN-Lite Specific),Compression Techniques Used,Unstructured Pruning (Neuron level) guided by XAI.,,,
V. Efficiency Metrics (PIN-Lite Specific),Edge Deployment Discussed,"Yes, for clinical settings with limited infrastructure.",,,
VI. Critical Analysis,Strengths,Novel link between interpretability and efficiency. distinct analysis of ViT vs CNN robustness.,,,
VI. Critical Analysis,Drawbacks & Limitations,Unstructured pruning often requires specialized hardware to realize speedups (vs structured pruning). Calculation of importance scores adds overhead.,,,
VI. Critical Analysis,Future Work,Validate on region-specific data for better generalization.,,,
VII. Relevance to PIN-Lite,Comparison Points,Provides a smart pruning metric (XAI-based) alternative to magnitude pruning for PIN-Lite.,,,
VII. Relevance to PIN-Lite,Key Differences from Our Approach,"Medical domain focus, but the pruning methodology is transferrable to Deepfake detection.",,,
VII. Relevance to PIN-Lite,Citation Context,Technique for 'Smart Pruning' to retain critical features in compressed models.,,,